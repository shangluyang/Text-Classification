{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQMxEixJ3XZj",
        "outputId": "a8448efc-724c-4b5e-b2b1-223138bacc34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "sys.path.append('/content/gdrive/MyDrive/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cqUcN-rU3opX"
      },
      "outputs": [],
      "source": [
        "SYSPATH = '/content/gdrive/MyDrive/data/'\n",
        "WORD_EMBEDDING_FILE = SYSPATH + 'sgns.weibo.word.bz2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwR6PDuYEIMG",
        "outputId": "ad3b3369-637d-45c4-dc26-816e260d3a8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2021"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "config = {\n",
        "    'train_file_path': SYSPATH + 'train.csv',\n",
        "    'test_file_path': SYSPATH + 'test.csv',\n",
        "    # 10% data are validation set\n",
        "    'train_val_ratio': 0.1,\n",
        "    'vocab_size': 30000,\n",
        "    'batch_size': 64,\n",
        "    'num_epochs': 10,\n",
        "    'learning_rate': 1e-3,\n",
        "    'logging_step': 100,\n",
        "    'seed': 2021\n",
        "}\n",
        "\n",
        "config['device'] = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-c6KBwsdEIMH"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import jieba\n",
        "\n",
        "def get_vocab(config):\n",
        "  token_counter = Counter()\n",
        "  with open(config['train_file_path'], 'r', encoding='utf8') as f:\n",
        "    lines = f.readlines()[1:]\n",
        "    for line in tqdm(lines, desc='Counting tokens', total=len(lines)):\n",
        "        sentence = line.split(',')[0].strip()\n",
        "        # seperate sentence\n",
        "        sentence_cut = list(jieba.cut(sentence))\n",
        "        token_counter.update(sentence_cut)\n",
        "  vocab = [token for token, _ in token_counter.most_common(config['vocab_size'])]\n",
        "  f.close()\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h-9E-7rEIMI",
        "outputId": "3bbbffda-ccbf-4d58-93bd-c8757b4e85fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCounting tokens:   0%|          | 0/180000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.048 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Counting tokens: 100%|██████████| 180000/180000 [00:29<00:00, 6015.63it/s]\n"
          ]
        }
      ],
      "source": [
        "vocab = get_vocab(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sMHgOcRaEIMJ"
      },
      "outputs": [],
      "source": [
        "# open word embedding\n",
        "import bz2\n",
        "with bz2.open(WORD_EMBEDDING_FILE) as f:\n",
        "  token_vector = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UPaR1E5q9VrZ"
      },
      "outputs": [],
      "source": [
        "# get word embedding for each word\n",
        "import random\n",
        "def get_embedding(vocab):\n",
        "  token2embedding = {}\n",
        "  voc_size, dim = token_vector[0].split()\n",
        "  print(f'{voc_size} tokens in embedding file, vector size is {dim}')\n",
        "  for line in tqdm(token_vector[1:]):\n",
        "    line = line.split() \n",
        "    token = line[0].decode('utf8')\n",
        "    vector = line[1:]\n",
        "    if token in vocab:\n",
        "      token2embedding[token] = list(map(float, vector))\n",
        "\n",
        "  # 4 special character\n",
        "  token2id = {token: id for id, token in enumerate(token2embedding.keys(), 4)}\n",
        "  id2embedding = {token2id[token]: embedding for token, embedding in token2embedding.items()}\n",
        "  UNK, PAD, BOS, EOD = '<unk> <pad> <bos> <eos>'.split()\n",
        "  token2id[PAD] = 0\n",
        "  token2id[UNK] = 1\n",
        "  token2id[BOS] = 2\n",
        "  token2id[EOD] = 3\n",
        "\n",
        "  id2embedding[0] = [0.] * int(dim)\n",
        "  id2embedding[1] = [0.] * int(dim)\n",
        "  id2embedding[2] = [random.uniform(-1, 1)] * int(dim)\n",
        "  id2embedding[3] = [random.uniform(-1, 1)] * int(dim)\n",
        "  embedding = [id2embedding[i] for i in range(len(id2embedding))]\n",
        "  return torch.tensor(embedding, dtype=torch.float), token2id, len(vocab) + 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIGS8lJjDzIs",
        "outputId": "948fb45c-0a61-4777-ea1b-b07b23d9dadb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'195202' tokens in embedding file, vector size is b'300'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195202/195202 [03:05<00:00, 1053.83it/s]\n"
          ]
        }
      ],
      "source": [
        "embedding, token2id, config['vocab_size'] = get_embedding(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Pz6MBg_C75z0"
      },
      "outputs": [],
      "source": [
        "# get tokenized id, use it to find corresponding word embedding\n",
        "def tokenizer(sentence, token2id):\n",
        "  ids = [token2id.get(token, 1) for token in jieba.cut(sentence)]\n",
        "  return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read dataset"
      ],
      "metadata": {
        "id": "dTx7ZDYUR8dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WdZV5lrAKXLs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# for training set, we split training set into training and validation set.\n",
        "def read_data(config, token2id, mode='train'):\n",
        "  df = pd.read_csv(config[mode + '_file_path'], sep=',')\n",
        "  if mode == 'train':\n",
        "    X_train, y_train = defaultdict(list), []\n",
        "    X_val, y_val = defaultdict(list), []\n",
        "    num_val = int(config['train_val_ratio'] * len(df))\n",
        "  else:\n",
        "    X_test, y_test = defaultdict(list), []\n",
        "\n",
        "  for i, row in df.iterrows():\n",
        "    label = row[1] if mode == 'train' else 0\n",
        "    sentence = row[0]\n",
        "    inputs = tokenizer(sentence, token2id)\n",
        "    if mode == 'train':\n",
        "      if i < num_val:\n",
        "        X_val['input_ids'].append(inputs)\n",
        "        y_val.append(label)\n",
        "      else:\n",
        "        X_train['input_ids'].append(inputs)\n",
        "        y_train.append(label)\n",
        "    else:\n",
        "      X_test['input_ids'].append(inputs)\n",
        "      y_test.append(label)\n",
        "\n",
        "  if mode == 'train':\n",
        "    label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    y_train = torch.tensor([label2id[label] for label in y_train], dtype=torch.long)\n",
        "    y_val = torch.tensor([label2id[label] for label in y_val], dtype=torch.long)\n",
        "    return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "  else:\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "    return X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_8fLIj14LvRB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class NEWSDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.x = X\n",
        "    self.y = y\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        'input_ids': self.x['input_ids'][idx],\n",
        "        'label': self.y[idx]\n",
        "        }\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HtW_Aos1W0xg"
      },
      "outputs": [],
      "source": [
        "def collete_fn(examples):\n",
        "  input_ids_list = []\n",
        "  labels = []\n",
        "  for example in examples:\n",
        "    input_ids_list.append(example['input_ids'])\n",
        "    labels.append(example['label'])\n",
        "  # find the longest sentence in input_ids_list\n",
        "  max_length = max(len(input_ids) for input_ids in input_ids_list)\n",
        "\n",
        "  # create tensor, dimension = sample_size * max_length\n",
        "  input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
        "  # fill tensor \n",
        "  for i, input_ids in enumerate(input_ids_list):\n",
        "    seq_len = len(input_ids)\n",
        "    input_ids_tensor[i, : seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
        "  return {'input_ids': input_ids_tensor,\n",
        "          'label': torch.tensor(labels, dtype=torch.long)} \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7EpgrXNsl-LZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def build_dataloader(config, vocab):\n",
        "  X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, token2id, mode='train')\n",
        "  X_test, y_test = read_data(config, token2id, mode='test')\n",
        "  train_dataset = NEWSDataset(X_train, y_train)\n",
        "  val_dataset = NEWSDataset(X_val, y_val)\n",
        "  test_dataset = NEWSDataset(X_test, y_test)\n",
        "\n",
        "  train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=True, collate_fn=collete_fn)\n",
        "  val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collete_fn)\n",
        "  test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collete_fn)\n",
        "  return id2label, train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHpRG2j2tOMa",
        "outputId": "93f23b44-708c-4139-8017-e786d5244bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "id2label, train_dataloader, val_dataloader, test_dataloader = build_dataloader(config, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0UZMmLD1kXV",
        "outputId": "62f2d1b2-5e9f-49d8-e4f4-e50e7979b68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1,  1324,  1939,  9114, 14634,   597,     1,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [22007,  8463,  7599,    90,   262,  2755,   150,   941,     1,     1,\n",
            "             1,  5836,     0,     0,     0],\n",
            "        [  179,    68,   895,    98,     1,  2693,  1974,     9,  7402, 23025,\n",
            "             1,  1169,  7334,     0,     0],\n",
            "        [ 4822,   744, 24815, 20036,     1,   298,  7613, 17256,  1138, 10636,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 1361,  6419,   842, 10498, 18963,  5120,     1,  6273, 11225, 22894,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [13683,  9036,  3619, 18593,     1, 15112,     1,  8108,  4396,  5048,\n",
            "          3940,     0,     0,     0,     0],\n",
            "        [ 7772,    54,     1,    86,  2238,     1, 14412,     9,    31,    65,\n",
            "            20,     5,   736,     0,     0],\n",
            "        [    1,     9,  3264,  2990,  2742,   551,  8097,    16,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [23799,     1,     1,     1,     1,  6208,  3231,   218,   441,  3162,\n",
            "           150,   441,   315,     0,     0],\n",
            "        [11872,   185,   230,   327,     1,    17,  2802,    21,   673,  2926,\n",
            "            35, 15113,   100,   280,   139],\n",
            "        [ 7339,    26,  8438,    17,     1,    21, 20422,     1,   546,  2877,\n",
            "          4784, 13104,  2076,  2166,     0],\n",
            "        [ 2153,     1,  3551,  8947,  1167,     1,  4589,  5451, 15491,     1,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [14074,    54,  1068,     1,     1,  1435,    46,   174,   211, 14074,\n",
            "           174,   161,     0,     0,     0],\n",
            "        [ 7692,     1,  3301,  1705, 15431, 13410,  1496, 24131,     1,  2385,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [   57, 18752, 16745,     1, 20963,     1,   114,    27,  1647,  1913,\n",
            "             1,     0,     0,     0,     0],\n",
            "        [ 1505, 10025,  5576, 17628,   821,  1739,  6066,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 6657,  5589,     1,  5459,     1,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [   39,  9650, 11682,    30,     1, 15236,  6821, 13649,   346,  5642,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [    1, 15299,     1,  3263,  5148,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [  159,   928,     9,  8160,  1410,  8158,     1,     1,     1,  9140,\n",
            "           127,  6280, 22461, 20073,     0],\n",
            "        [ 2753, 14529, 14516,  4976, 22920, 11139,   966,    24,  1831,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 4467,  6476,     9, 16622,  6574, 22914,     1, 12911,     1,   846,\n",
            "             1,  4730,     0,     0,     0],\n",
            "        [ 8119,  1063,   429,    14,  5945, 12416,     5, 12089,   173,  6643,\n",
            "           505,    14,  5945, 17357,     0],\n",
            "        [  182,    39,     1, 20209,    30,    17,  5454,    21,  3364,  5864,\n",
            "             1,  4658,  1291,   372,     0],\n",
            "        [    1,     1,     1,     1,   400, 10807,  2585,  5498,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [    1,     9, 20963,  7504,     1, 16395,  2552,  2367,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 6122, 21737,  6189,    39,     1,  5604, 14342,    30,   294,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [10891,    18,   862,  1440, 15387,   196,    17,     1,    21,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [  489,  1670,     1,    34, 21493,  3549,   236,   100,     1,   139,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [  731,     1,     9,   174,    14,  2559,   248,    45,    57,     5,\n",
            "             1,     1,     0,     0,     0],\n",
            "        [ 1539,     1,  5193,  1714,  5623,  2437,     1, 22540,  2039,     1,\n",
            "             1,     0,     0,     0,     0],\n",
            "        [ 4035,     1,     9,  1183,  8596, 11153,     1, 11751,    13,  5131,\n",
            "          8588, 10039,     0,     0,     0],\n",
            "        [15757,  1522, 21645,   234,  4949,  1497,    34,  4083,  3740,   182,\n",
            "         14120,   781,   120,   941,     0],\n",
            "        [16820,  4945,   347,   272,     1,   213,    41,     1,  7449,   838,\n",
            "           593,   100,   280,   139,     0],\n",
            "        [ 2153,  1381,  5902,  1600,  4702, 11299, 11383,  4176,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 4829, 18308,    73,    14,  4380,     1,     1,  1169,   400,  9597,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 6973, 12388,  5496,   114, 19464, 10806,     1, 11258, 14630,   266,\n",
            "         24711, 18688,     0,     0,     0],\n",
            "        [16001,   434, 15507,  2434,     1,     1, 20979,   346,    35,  1800,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [19075, 15039,    60,   153,   120,    68,  1805,     1,    14, 20308,\n",
            "          1449,  3200,     0,     0,     0],\n",
            "        [19264,  5663, 11542,     1,  6763,     1,  1045,     1,   202,     1,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 2821,  1063, 10886,  8024,    41,   245, 18847,    12,    91,   245,\n",
            "         22905,     0,     0,     0,     0],\n",
            "        [ 5998, 21453, 10374,     1, 21359,  2490, 21860,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [  159,   150,  2276,  6374,     1,     1,  1097,     1,  3415,   120,\n",
            "          2276,    55,   884,     0,     0],\n",
            "        [ 8007, 20266, 20616,     1, 14950,   441,     1,     1,     9,  9337,\n",
            "          2315,   329,   380,     0,     0],\n",
            "        [15466,  4863, 12400,     1, 11682,     5,    24,    29,     1,    28,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [    1, 16693,  3740, 10294,  1568, 21785, 13410, 17817, 24131,   100,\n",
            "           280,   139,     0,     0,     0],\n",
            "        [    1,  6988,     9, 23234,     1, 16583,  4870, 11167,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [14277, 24399,  6618, 10620,     1,  6122,     1,   879, 16615,   144,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 3473, 17536,     1, 13169, 24668,   144,  7128,  1048,   781,   100,\n",
            "             1,   139,     0,     0,     0],\n",
            "        [  327, 17567,    45,  1419,   887,  1334,  9407, 10071,     1,  2500,\n",
            "            54,  4549, 21619,     0,     0],\n",
            "        [13394,  6468, 23432,    60,   153,   114,    68,  4418,  2306,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [  989,  7571,  3638,  4628,   266,   917,  3770,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 1160,   365, 14029,     1,   296,   779,    54, 19116,   150,   941,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [    1,   438,  4309,  2140,  2268, 13377, 13272,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 1029,  8082,  4624, 19029,     1,     1,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [    1,  4744,     1,  9497,    54, 13367,  2092,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 4149,     1,  9316,     1,   749,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [14885,  1939,  1259,  2761,     1, 11891,  5514,     1,  1954, 14074,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [23162,    52, 20026,  1939,    60,  3832,     1,     1,    35, 20503,\n",
            "           969,   747,     0,     0,     0],\n",
            "        [ 8987,    39,  1733,   331,    30,   555,  5674,  6925,  2118,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [13419,  9815,  7440,    90,    80,  1818,     1,     1,     1,   874,\n",
            "            48,    90, 12937,     0,     0],\n",
            "        [  819,  4077, 12513,  8566,   234, 22053,     1,  9428,     1, 16963,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 2700,  3127,   137,  6215,     1,  1757,  2364, 16067,   215,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [ 1445,  2537,   429,   743,  4815,  3020,   424,     1,   428,  8955,\n",
            "          5434,     0,     0,     0,     0]])\n"
          ]
        }
      ],
      "source": [
        "# sentence -> [word1, word2, word3] -> [id1, id2, id3]\n",
        "for batch in train_dataloader:\n",
        "  print(batch['input_ids'])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, token2id, mode='train')\n",
        "X_test, y_test = read_data(config, token2id, mode='test')"
      ],
      "metadata": {
        "id": "ar5w9jkQ9gMa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x0nKXWDsEIMK"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    'embedding_pretrained' : embedding,\n",
        "    'num_filters' : 256,\n",
        "    'emb_size' : embedding.shape[1],\n",
        "    'dropout' : 0.3,\n",
        "    'filter_sizes' : [2,3,5],\n",
        "    'num_classes' : len(label2id)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextCNN"
      ],
      "metadata": {
        "id": "pE3fc10qZubj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EX0USxa0EIML"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "      super(Model, self).__init__()\n",
        "      # create embedding\n",
        "      self.embedding = nn.Embedding.from_pretrained(config['embedding_pretrained'].float(), freeze=True)\n",
        "      # convolution layer\n",
        "      self.convs = nn.ModuleList([nn.Conv2d(1, config['num_filters'], (k, config['emb_size'])) for k in config['filter_sizes']])\n",
        "      # add dropout\n",
        "      self.dropout = nn.Dropout(config['dropout'])\n",
        "      # linear layer\n",
        "      self.fc = nn.Linear(config['num_filters'] * len(config['filter_sizes']), config['num_classes'])\n",
        "\n",
        "    def convs_and_pool(self, x, conv):\n",
        "\n",
        "        x = F.relu(conv(x)).squeeze(3)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_ids=None, label=None):\n",
        "        # out [batch_size, seq_len, embedding_dim]\n",
        "        out = self.embedding(input_ids)\n",
        "        \n",
        "        # H: seq_len; W:embedding_dim\n",
        "        # out [batch_size, 1, seq_len, embedding_dim]\n",
        "        out = out.unsqueeze(1)\n",
        "\n",
        "        # (batch_size, out_channels)\n",
        "        out = torch.cat([self.convs_and_pool(out, conv) for conv in self.convs], 1)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        output = (out, )\n",
        "\n",
        "        # During training\n",
        "        if label is not None: \n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(out, label)\n",
        "            output = (loss, ) + output\n",
        "\n",
        "        # train output (loss, out)\n",
        "        # test output (out)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WWiOyJk8EIMM"
      },
      "outputs": [],
      "source": [
        "model = Model(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model"
      ],
      "metadata": {
        "id": "1pe9VbGfsVay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "vMcQU0G-sUfr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "def evaluation(config, model, val_dataloader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    val_loss =0.\n",
        "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "    with torch.no_grad():\n",
        "        for batch in val_iterator:\n",
        "            labels.append(batch['label'])\n",
        "            batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "\n",
        "            # val output (loss, out)\n",
        "            loss, logits = model(**batch)[:2]\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "    \n",
        "    avg_val_loss = val_loss/len(val_dataloader)\n",
        "    labels = torch.cat(labels, dim=0).numpy()\n",
        "    preds = torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    recall = recall_score(labels, preds, average='macro')\n",
        "    precision = precision_score(labels, preds, average='macro')\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "    return [avg_val_loss, accuracy, recall, precision, f1]"
      ],
      "metadata": {
        "id": "m7WCSebCuxdO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CoXxwVCnEIMM"
      },
      "outputs": [],
      "source": [
        "def train(model, config, id2label, train_dataloader, val_dataloader):\n",
        "  optimer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "  model.to(config['device'])\n",
        "  \n",
        "  global_step = 0\n",
        "  train_loss = 0\n",
        "  logging_loss = 0\n",
        "  accuracys = []\n",
        "\n",
        "  for epoch in range(config['num_epochs']):\n",
        "    train_iterator = train_dataloader\n",
        "    model.train()\n",
        "    for batch in train_iterator:\n",
        "      batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "      # train output (loss, out)\n",
        "      loss = model(**batch)[0]\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimer.step()\n",
        "      train_loss += loss\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % config['logging_step'] == 0:\n",
        "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "                logging_loss = train_loss\n",
        "                result = evaluation(config, model, val_dataloader)\n",
        "                avg_val_loss, accuracy = result[0], result[1]\n",
        "                accuracys.append(accuracy)\n",
        "                print_log = f'>>> training loss: {print_train_loss:.4f}, valid loss: {avg_val_loss:.4f}, valid accuracy: {accuracy:.4f}'\n",
        "                print(print_log)\n",
        "                model.train()\n",
        "\n",
        "    return model, accuracys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TkRZiSkJEIMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6457f0-95f6-4ea1-c07a-a8078235b7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 1.1033, valid loss: 0.6642, valid accuracy: 0.7899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:19<00:00, 14.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.5725, valid accuracy: 0.8107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.5139, valid accuracy: 0.8342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.5047, valid accuracy: 0.8371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4837, valid accuracy: 0.8451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4672, valid accuracy: 0.8476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4495, valid accuracy: 0.8546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 14.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4478, valid accuracy: 0.8533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:19<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4350, valid accuracy: 0.8593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:19<00:00, 14.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4271, valid accuracy: 0.8631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 14.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4249, valid accuracy: 0.8617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4402, valid accuracy: 0.8550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4198, valid accuracy: 0.8665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4246, valid accuracy: 0.8626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4080, valid accuracy: 0.8649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4105, valid accuracy: 0.8662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4053, valid accuracy: 0.8675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4006, valid accuracy: 0.8676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4070, valid accuracy: 0.8645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4031, valid accuracy: 0.8680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.4080, valid accuracy: 0.8671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.3903, valid accuracy: 0.8734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:18<00:00, 15.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.3989, valid accuracy: 0.8689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:19<00:00, 14.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.3908, valid accuracy: 0.8722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/282 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 282/282 [00:19<00:00, 14.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> training loss: 0.0000, valid loss: 0.3958, valid accuracy: 0.8697\n"
          ]
        }
      ],
      "source": [
        "best_model, accuracys = train(model, config, id2label, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1JbhDVT3r5c",
        "outputId": "5a29c2e7-2817-4dd4-998a-55ce2eb25e51"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7899444444444444,\n",
              " 0.8106666666666666,\n",
              " 0.8342222222222222,\n",
              " 0.8371111111111111,\n",
              " 0.8451111111111111,\n",
              " 0.8476111111111111,\n",
              " 0.8545555555555555,\n",
              " 0.8533333333333334,\n",
              " 0.8593333333333333,\n",
              " 0.8631111111111112,\n",
              " 0.8616666666666667,\n",
              " 0.855,\n",
              " 0.8665,\n",
              " 0.8626111111111111,\n",
              " 0.8649444444444444,\n",
              " 0.8662222222222222,\n",
              " 0.8675,\n",
              " 0.8675555555555555,\n",
              " 0.8645,\n",
              " 0.868,\n",
              " 0.8670555555555556,\n",
              " 0.8734444444444445,\n",
              " 0.8688888888888889,\n",
              " 0.8721666666666666,\n",
              " 0.8697222222222222]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_ksq3LiHEIMN"
      },
      "outputs": [],
      "source": [
        "def predict(config, id2label, model, test_dataloader):\n",
        "    test_iterator = tqdm(test_dataloader, desc='Evaluation', total=len(test_dataloader))\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iterator:\n",
        "            batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "\n",
        "            logits = model(**batch)[1]\n",
        "\n",
        "            test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "    \n",
        "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
        "    test_preds = [id2label[id_] for id_ in test_preds]\n",
        "\n",
        "    test_df = pd.read_csv(config['test_file_path'], sep=',')\n",
        "    test_df.insert(1, column='predicted_label', value=test_preds)\n",
        "    test_df.drop(columns=['sentence'], inplace=True)\n",
        "    return test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predict(config, id2label, best_model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR-4M0qr0suh",
        "outputId": "9e735a27-1732-474f-9601-f5c14b38dd09"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Evaluation: 100%|██████████| 157/157 [00:09<00:00, 16.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(prediction['predicted_label'], prediction['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlUMVb1f7A2h",
        "outputId": "6a8f1074-fdc0-4e97-cf7f-67a884cb9f04"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8704"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(prediction['predicted_label'], prediction['label'], average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYlXhVTh7M7J",
        "outputId": "6e0a2d34-2a3d-447b-a26b-4c238f46dfac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8704"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(prediction['predicted_label'], prediction['label'], average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx_v5qUy7WHL",
        "outputId": "3fbd9cfc-c076-495d-a680-39a16de6040d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8753419"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(prediction['predicted_label'], prediction['label'], average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR7LVBRZ7YmA",
        "outputId": "96669d11-f3a9-4071-9d1c-c739706a8201"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8707552758742632"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "textcnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
      "language": "python",
      "name": "py35-paddle1.2.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}